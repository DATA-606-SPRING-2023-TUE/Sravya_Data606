{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzSc6tJVYMR3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en"
      ],
      "metadata": {
        "id": "8jV3b2ZYUYZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os  \n",
        "import pandas as pd \n",
        "import spacy  # we use spacy for implementation of tokenizer.\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence  # padding of batch.\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image  # Load imgage\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "lG4origrUfcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_eng = spacy.load('en_core_web_sm') # to know tokenizer it is working with."
      ],
      "metadata": {
        "id": "u8CMh2lQUh-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"} # Padd Token , Start of sentence, End of sentence, Unknown.\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self): # getting length of our vocabulory.\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)] # we get lower case of the tokenizer of text we send.\n",
        "        # example:>  \"Get along soon\" -> [\"get\",\"along\",\"soon\"]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list): # used to go through each of text and see if its over the threshold and if so we ignore it.\n",
        "        frequencies = {}\n",
        "        idx = 4  # we are starting with an index of 4 because we already included first three.\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                if word not in frequencies: \n",
        "                    frequencies[word] = 1\n",
        "\n",
        "                else:\n",
        "                    frequencies[word] += 1 \n",
        "\n",
        "                if frequencies[word] == self.freq_threshold: # here we see if frequency of word is equad to the threshold frequency.\n",
        "                    self.stoi[word] = idx # So we set the index starting at 4.\n",
        "                    self.itos[idx] = word # and we set word into that index.\n",
        "                    idx += 1 # we increment the index.\n",
        "\n",
        "    def numericalize(self, text): # we take the sentence and convert them to numerical values.\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] # if token are in stoi, then it surparses the frequency threshold.\n",
        "                                             #Else it wouldnt be in self.toi and we just return the index of unkown token.\n",
        "            for token in tokenized_text\n",
        "        ]"
      ],
      "metadata": {
        "id": "8_emZVCBUka_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlickrDataset(Dataset): # Talking the class dataset.\n",
        "    \n",
        "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5): # root directory of image is passed with caption file and a fequency threshold.\n",
        "        self.root_dir = root_dir # getting the root directory, in our case, we have flickr8k folder.\n",
        "        self.df = pd.read_csv(captions_file) # reading the captions from caption file.\n",
        "        self.transform = transform # \n",
        "\n",
        "        \n",
        "        self.imgs = self.df[\"image\"]  # we get the image from image column.\n",
        "        self.captions = self.df[\"caption\"] # we get the caption assosiated with image from image column.\n",
        "\n",
        "        \n",
        "        self.vocab = Vocabulary(freq_threshold) # Initialize vocabulary with respect to threshold we specified.\n",
        "        self.vocab.build_vocabulary(self.captions.tolist()) # We build the vocabulory here and the captions is passed as a list into the function's parameters.\n",
        "\n",
        "    def __len__(self): # we get length of dataframe here.\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index): # we use to get a single example with corresponding caption.\n",
        "        caption = self.captions[index]\n",
        "        img_id = self.imgs[index]\n",
        "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\") # loading of image.\n",
        "\n",
        "        if self.transform is not None: # in case we have a stransform we can use it.\n",
        "            img = self.transform(img)\n",
        "\n",
        "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]] # string to index -> index of start token.\n",
        "        numericalized_caption += self.vocab.numericalize(caption) # we numericalize the caption.\n",
        "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"]) # append end of sentence.\n",
        "\n",
        "        return img, torch.tensor(numericalized_caption) #return image by converting numnericalized caption to tensor."
      ],
      "metadata": {
        "id": "D-tS8sdfUpPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx # getting the padd index.\n",
        "\n",
        "    def __call__(self, batch): # we have batch, which is list of all examples we have.\n",
        "        # unsqueze -> Returns a new tensor with a dimension of size one inserted at the specified position.\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch] # 1st item returned for each item in batch.\n",
        "        # torch.cat -> Concatenates the given sequence of seq tensors in the given dimension. \n",
        "        imgs = torch.cat(imgs, dim=0) #Concates the images we unsquezed to given dimension. All images must be of same size.\n",
        "        targets = [item[1] for item in batch] # targets are the captions.\n",
        "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx) # targets are papdded with pad_sequence function.\n",
        "        # if batch_first = True,  If True, then the input and output tensors are provided as (batch, seq, feature).\n",
        "\n",
        "        return imgs, targets # images and targets are returned."
      ],
      "metadata": {
        "id": "nVTcyqhROyae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(  # loads everything for us.\n",
        "    root_folder,\n",
        "    annotation_file,\n",
        "    transform,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
        "\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "    )\n",
        "\n",
        "    return loader, dataset\n",
        "\n",
        "    \n",
        "    transform = transforms.Compose(\n",
        "        [transforms.Resize((224, 224)), transforms.ToTensor(),]\n",
        "    )\n",
        "\n",
        "    loader, dataset = get_loader(\n",
        "        \"/content/drive/MyDrive/DataSet/images\", \"/content/drive/MyDrive/DataSet/captions.txt\", transform=transform\n",
        "    )\n",
        "\n",
        "    for idx, (imgs, captions) in enumerate(loader):\n",
        "        print(\"index number: \",idx)\n",
        "        print(\"Shape of image is: \",imgs.shape)\n",
        "        print(\"numericalized captions: \",captions)\n",
        "        print(\"Shape of captions: \",captions.shape)"
      ],
      "metadata": {
        "id": "fKZjIttMO2c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import statistics\n",
        "import torchvision.models as models ## used to load the pytorch models for vision\n",
        "\n",
        "\n",
        "class EncoderCNN(nn.Module): ## encoder class used for the CNN part\n",
        "    def __init__(self, embed_size, train_CNN=False):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        self.train_CNN = train_CNN  ## we use just a pre-trained model.\n",
        "        self.inception = models.inception_v3(pretrained=True) ## we use the inception model.\n",
        "        #self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size) ## fully connected, access last linear layer and replace it with linear and map it to embed size.\n",
        "        #self.relu = nn.ReLU()\n",
        "        #self.times = []\n",
        "        #self.dropout = nn.Dropout(0.5, inplace = True)\n",
        "        #self.inception = nn.Sequential(*list(self.inception.children())[:-1])  \n",
        "        #num_classes = 10  # Example: 10 classes for classification\n",
        "        #self.classifier = nn.Linear(2048, embed_size)  \n",
        "        #self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.inception.aux_logits = False\n",
        "        self.inception.fc = nn.Linear(2048, embed_size)\n",
        "        #self.relu = nn.ReLU()\n",
        "        #self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "    def forward(self, images): ## take input image and compute features with inception of images.\n",
        "        features = self.inception(images)\n",
        "        return features\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size) # we need embedding here to map our word to get better representation of word.\n",
        "                                                          # It will take an index and map into some embed size.\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers) ## LSTM model is build here.\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, features, captions): ## features and target caption in dataset.\n",
        "        embeddings = self.dropout(self.embed(captions))\n",
        "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0) # concatinate the features with the embedding and on dimension 0..\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class CNNtoRNN(nn.Module): # cnn to rnn is hooked here.\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(CNNtoRNN, self).__init__()\n",
        "        self.encoderCNN = EncoderCNN(embed_size)\n",
        "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoderCNN(images)\n",
        "        outputs = self.decoderRNN(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocabulary, max_length=50):\n",
        "        result_caption = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = self.encoderCNN(image).unsqueeze(0)\n",
        "            states = None\n",
        "\n",
        "            for _ in range(max_length): ## upto the max_length words prediction, 50 here.\n",
        "                hiddens, states = self.decoderRNN.lstm(x, states)  ## at beginning it will be initialized as 0.\n",
        "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
        "                predicted = output.argmax(1) # so we taking word with higgest probablity.\n",
        "                result_caption.append(predicted.item())\n",
        "                x = self.decoderRNN.embed(predicted).unsqueeze(0) # taking the predicted word.\n",
        "\n",
        "                if vocabulary.itos[predicted.item()] == \"<EOS>\": # check if vocab is equal to end of sentence then break.\n",
        "                    break\n",
        "\n",
        "        return [vocabulary.itos[idx] for idx in result_caption]"
      ],
      "metadata": {
        "id": "mPTlh80VO8jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QMylThGeka2c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}